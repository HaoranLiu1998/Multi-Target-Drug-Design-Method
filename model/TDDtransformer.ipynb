{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40132e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02561e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda\n"
     ]
    }
   ],
   "source": [
    "#Set device 有GPU则device=GPU，否则=CPU\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    device = torch.device(\"cuda\")\n",
    "    cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    cuda = False\n",
    "    \n",
    "print(\"Device =\", device)\n",
    "gpus = [0] #不太明白这一句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c435eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算 当前与选取参数start_time时的时间差，返回三个int型， 小时，分钟，秒\n",
    "def time_elapsed(start_time):\n",
    "    elapsed = time.time() - start_time  #.time()单位是浮点数的'秒'，时间间隔elapsed也是以浮点数“秒”来表示\n",
    "    hours = int(elapsed/3600)           #elapsed/3600获得 小时数（自动对下取整\n",
    "    minutes = int(int(elapsed/60)%60)   #elapsed/60获得分钟数，然后%60对60取余，获得 分钟数\n",
    "    seconds = int(elapsed%60)           #elapsed%60对秒取余，获得 秒数\n",
    "    \n",
    "    return hours, minutes, seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a509888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein2int(protein, voc):\n",
    "    \n",
    "    protein_int= [ ]\n",
    "    for char in protein:\n",
    "        protein_int.append([float(i) for i, x in enumerate(voc) if x==char][0])\n",
    "\n",
    "    protein_int = np.array(protein_int).astype(float)\n",
    "    return protein_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b683ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDDTransformerEncoder (nn.Module):\n",
    "    def __init__(self, nembedding=22, ninp=1024, nhead=8, nhid=1024, nlayers=3, dropout=0.2):\n",
    "        super(TDDTransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(nembedding, ninp)\n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout,batch_first=True)#输入数据格式（batch，seqlenth，dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layers, nlayers)\n",
    "        \n",
    "        \n",
    "    def forward(self,protein):\n",
    "        output1 = self.embedding(protein)#(batch,seq_len,embedding)\n",
    "        output1 = output1.unsqueeze(0)##(batch,seq_len,ninp)\n",
    "        #print(\"output1 size\",output1.size())\n",
    "        output2 = self.transformer_encoder(output1)#(batch,seq_len,hidden_zise)\n",
    "        protein_vector = torch.unsqueeze(torch.mean(output2, 1), 0)#(batch,1,hidden_zise)\n",
    "        return protein_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "647c24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDDTransformerDecoder (nn.Module):\n",
    "    def __init__(self, nembedding=21, ninp=1024, nhead=8, nhid=1024, nlayers=3, dropout=0.2):\n",
    "        super(TDDTransformerDecoder, self).__init__()\n",
    "        self.hidden_size = nhid/2\n",
    "        self.laryer = nlayers - 1\n",
    "        self.Lstm = nn.LSTM(input_size = 35, hidden_size = 1024, num_layers = 3, batch_first = True, dropout = 0.2)\n",
    "        self.linear = nn.Linear(1024, 35, bias = True)\n",
    "        self.Relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,protein_vec, smiles, hidden): #对于一个靶点，每次传入相同protein_vec，不同的inp。逐个生成SMILES字符\n",
    "        protein_vec = protein_vec.view(1,1,-1)#(1,1,batch*hidden_zise) batch = 1\n",
    "        protein_vec = torch.cat((protein_vec,protein_vec,protein_vec),0)#(3,1,batch*hidden_zise)\n",
    "        #hidden = protein_vec + hidden\n",
    "        #print(\"protein_vec size\",protein_vec.size())\n",
    "        \n",
    "        hidden = [hidden[0] + protein_vec,\n",
    "                  hidden[1]] #hidden （num_layers, batch_size, hidden_size）\n",
    "        smiles = smiles.unsqueeze(0)##(batch,seq_len,ninp)\n",
    "        output1, hidden = self.Lstm(smiles, hidden)#hidden包括（h,c）,h隐藏状态（短期记忆），c记忆细胞（长期记忆）\n",
    "        output = self.linear(output1)\n",
    "        #每一次forward，都经历一边lstm+linear，返回计算结果output和隐藏层的states。output和input大小相同\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95981b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDDTransformer (nn.Module):\n",
    "    def __init__(self, nembedding=21, ninp=256, nhead=8, nhid=256, nlayers=3, dropout=0.2):\n",
    "        super(TDDTransformer, self).__init__()\n",
    "        self.Encoder = TDDTransformerEncoder()\n",
    "        self.Decoder = TDDTransformerDecoder()\n",
    "    \n",
    "    def init_states(self, batch_size = 1):\n",
    "\n",
    "        hidden = [Variable(torch.zeros(3, batch_size, 1024)),\n",
    "                  Variable(torch.zeros(3, batch_size, 1024))]\n",
    "\n",
    "\n",
    "\n",
    "        return hidden\n",
    "        \n",
    "    def forward(self, protein, smiles, hidden):#protein可以是一个protein，也可以是protein_vec。不同输入应对不同输出\n",
    "        Encoder_output = self.Encoder(protein)#(batch,seq_len,embedding)\n",
    "        Decoder_output = self.Decoder(Encoder_output, smiles, hidden)#(batch,seq_len,hidden_zise)\n",
    "        return Decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e204479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ep,No_data):\n",
    "    #Set start time\n",
    "    start_time = time.time()\n",
    "    train_loss = []\n",
    "    \n",
    "    #Iterate set of seq_length characters 序列长度字符的迭代集\n",
    "    for i, data in enumerate(train_loader):#所有的smiles都学一遍      \n",
    "        #Initialize hidden and cell states(初始化\n",
    "        oupt, inpt  = data\n",
    "        protein_inpt = protein_int_data[i]\n",
    "        protein_inpt = torch.from_numpy(protein_inpt)\n",
    "        \n",
    "        input_data = inpt.float()\n",
    "        target_data = oupt.long()\n",
    "        protein_inpt = protein_inpt.long()\n",
    "\n",
    "        protein_inpt = protein_inpt.to(device)\n",
    "        input_data = input_data.to(device)\n",
    "        target_data = target_data.to(device)\n",
    "        \n",
    "        hidden = TDDTnet.init_states()\n",
    "\n",
    "        hidden = (hidden[0].cuda(), hidden[1].cuda())\n",
    "        \n",
    "        #Set initial gradients\n",
    "        #初始化梯度\n",
    "        TDDTnet.zero_grad()\n",
    "    \n",
    "        #Set initial loss\n",
    "        #初始化损失函数\n",
    "        loss = 0 \n",
    " \n",
    "        #Run model, calculate loss\n",
    "        #forward一遍\n",
    "        output, hidden= TDDTnet(protein_inpt, input_data, hidden)\n",
    "        #计算一边loss，累加\n",
    "#         print(\"output size\",output.size())\n",
    "#         print(\"target size\",target_data.size())       \n",
    "        loss += criterion(output.view(-1,35), target_data.view(-1))    #output.view(-1,LSTM_output_size)\n",
    "    \n",
    "        train_loss.append(loss)\n",
    "        #Backpropagate loss\n",
    "        #梯度反传\n",
    "        loss.backward()\n",
    "        \n",
    "        #Clip gradients\n",
    "        #梯度裁剪，防止梯度过大\n",
    "        nn.utils.clip_grad_norm_(TDDTnet.parameters(), 3.0)\n",
    "        \n",
    "        #Optimize\n",
    "        #优化，参数根据梯度和学习率学一边\n",
    "        optimizer.step()\n",
    "            \n",
    "        #Update list of losses\n",
    "        #更新损失列表，每50个batch记录一次\n",
    "        if (i % 50 == 0):\n",
    "            losses[0] = loss.data.item() / seq_length   \n",
    "        losses.append(loss.data.item() / seq_length)\n",
    "\n",
    "        #Intermediary saves\n",
    "        #中间存储\n",
    "        if (i % 10000 == 0):\n",
    "                torch.save(TDDTnet.state_dict(), f\"TDDTv4_temp.pth\")\n",
    "\n",
    "        #Print training info\n",
    "        #输出训练信息\n",
    "        hours, minutes, seconds = time_elapsed(start_time)\n",
    "        print(\"epoch \",ep,\" dataset \",No_data, \" training \" + \"Loss: {:0.6f}\".format(loss.data.item() / seq_length) + \" | ΔLossTotal: {:+0.4f}\".format(losses[-1] - losses[1]) + \" | Iteration: {0:04d}\".format(i + 1) + \" | Time elapsed: {0:02d}\".format(hours) + \"h {0:02d}\".format(minutes) + \" m {0:02d}\".format(seconds) + \" s\")\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "085c9c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "protein_all = open('../data/train_protein3.txt', \"r\").read()\n",
    "protein_voc = list(set(protein_all))\n",
    "protein_voc_num = len(protein_voc) \n",
    "protein_all = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58e3ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in TTDT: 46404643\n"
     ]
    }
   ],
   "source": [
    "TDDTnet = TDDTransformer()\n",
    "optimizer = torch.optim.Adam(TDDTnet.parameters(), lr = 0.0001)\n",
    "criterion = nn.CrossEntropyLoss()#使用交叉熵损失函数\n",
    "seq_length = 76\n",
    "\n",
    "losses = [0]\n",
    "\n",
    "if cuda:\n",
    "    TDDTnet.cuda()\n",
    "    criterion.cuda()\n",
    "\n",
    "total_params = sum(p.numel() for p in TDDTnet.parameters())\n",
    "    \n",
    "print(\"Total number of parameters in TTDT: \" + str(total_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9638e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start the epoch 0 training!\n",
      "prepare protein data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 46015/46015 [00:32<00:00, 1417.45it/s]\n",
      "C:\\Users\\lhr\\AppData\\Local\\Temp\\ipykernel_6240\\952655365.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  protein_int_data = np.array([protein2int(i, protein_voc) for i in tqdm(protein_data)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data size: torch.Size([3451125, 35])\n",
      "output_data size: torch.Size([3451125])\n",
      "epoch  0  dataset  0  training Loss: 0.046647 | ΔLossTotal: +0.0000 | Iteration: 0001 | Time elapsed: 00h 00 m 00 s\n",
      "epoch  0  dataset  0  training Loss: 0.046421 | ΔLossTotal: -0.0002 | Iteration: 0002 | Time elapsed: 00h 00 m 01 s\n",
      "epoch  0  dataset  0  training Loss: 0.046168 | ΔLossTotal: -0.0005 | Iteration: 0003 | Time elapsed: 00h 00 m 01 s\n",
      "epoch  0  dataset  0  training Loss: 0.046218 | ΔLossTotal: -0.0004 | Iteration: 0004 | Time elapsed: 00h 00 m 01 s\n",
      "epoch  0  dataset  0  training Loss: 0.045929 | ΔLossTotal: -0.0007 | Iteration: 0005 | Time elapsed: 00h 00 m 01 s\n",
      "epoch  0  dataset  0  training Loss: 0.045611 | ΔLossTotal: -0.0010 | Iteration: 0006 | Time elapsed: 00h 00 m 01 s\n",
      "epoch  0  dataset  0  training Loss: 0.045162 | ΔLossTotal: -0.0015 | Iteration: 0007 | Time elapsed: 00h 00 m 01 s\n",
      "epoch  0  dataset  0  training Loss: 0.045140 | ΔLossTotal: -0.0015 | Iteration: 0008 | Time elapsed: 00h 00 m 01 s\n",
      "epoch  0  dataset  0  training Loss: 0.044849 | ΔLossTotal: -0.0018 | Iteration: 0009 | Time elapsed: 00h 00 m 01 s\n",
      "epoch  0  dataset  0  training Loss: 0.043955 | ΔLossTotal: -0.0027 | Iteration: 0010 | Time elapsed: 00h 00 m 01 s\n",
      "epoch  0  dataset  0  training Loss: 0.043452 | ΔLossTotal: -0.0032 | Iteration: 0011 | Time elapsed: 00h 00 m 02 s\n",
      "epoch  0  dataset  0  training Loss: 0.044315 | ΔLossTotal: -0.0023 | Iteration: 0012 | Time elapsed: 00h 00 m 02 s\n",
      "epoch  0  dataset  0  training Loss: 0.042486 | ΔLossTotal: -0.0042 | Iteration: 0013 | Time elapsed: 00h 00 m 02 s\n",
      "epoch  0  dataset  0  training Loss: 0.041458 | ΔLossTotal: -0.0052 | Iteration: 0014 | Time elapsed: 00h 00 m 02 s\n",
      "epoch  0  dataset  0  training Loss: 0.039405 | ΔLossTotal: -0.0072 | Iteration: 0015 | Time elapsed: 00h 00 m 02 s\n",
      "epoch  0  dataset  0  training Loss: 0.036895 | ΔLossTotal: -0.0098 | Iteration: 0016 | Time elapsed: 00h 00 m 02 s\n",
      "epoch  0  dataset  0  training Loss: 0.037404 | ΔLossTotal: -0.0092 | Iteration: 0017 | Time elapsed: 00h 00 m 02 s\n",
      "epoch  0  dataset  0  training Loss: 0.028004 | ΔLossTotal: -0.0186 | Iteration: 0018 | Time elapsed: 00h 00 m 02 s\n",
      "epoch  0  dataset  0  training Loss: 0.027510 | ΔLossTotal: -0.0191 | Iteration: 0019 | Time elapsed: 00h 00 m 03 s\n",
      "epoch  0  dataset  0  training Loss: 0.034182 | ΔLossTotal: -0.0125 | Iteration: 0020 | Time elapsed: 00h 00 m 03 s\n",
      "epoch  0  dataset  0  training Loss: 0.037985 | ΔLossTotal: -0.0087 | Iteration: 0021 | Time elapsed: 00h 00 m 03 s\n",
      "epoch  0  dataset  0  training Loss: 0.025139 | ΔLossTotal: -0.0215 | Iteration: 0022 | Time elapsed: 00h 00 m 03 s\n",
      "epoch  0  dataset  0  training Loss: 0.023724 | ΔLossTotal: -0.0229 | Iteration: 0023 | Time elapsed: 00h 00 m 03 s\n",
      "epoch  0  dataset  0  training Loss: 0.025263 | ΔLossTotal: -0.0214 | Iteration: 0024 | Time elapsed: 00h 00 m 03 s\n",
      "epoch  0  dataset  0  training Loss: 0.029925 | ΔLossTotal: -0.0167 | Iteration: 0025 | Time elapsed: 00h 00 m 03 s\n",
      "epoch  0  dataset  0  training Loss: 0.022888 | ΔLossTotal: -0.0238 | Iteration: 0026 | Time elapsed: 00h 00 m 03 s\n",
      "epoch  0  dataset  0  training Loss: 0.026321 | ΔLossTotal: -0.0203 | Iteration: 0027 | Time elapsed: 00h 00 m 04 s\n",
      "epoch  0  dataset  0  training Loss: 0.028307 | ΔLossTotal: -0.0183 | Iteration: 0028 | Time elapsed: 00h 00 m 04 s\n",
      "epoch  0  dataset  0  training Loss: 0.022161 | ΔLossTotal: -0.0245 | Iteration: 0029 | Time elapsed: 00h 00 m 04 s\n",
      "epoch  0  dataset  0  training Loss: 0.024922 | ΔLossTotal: -0.0217 | Iteration: 0030 | Time elapsed: 00h 00 m 04 s\n",
      "epoch  0  dataset  0  training Loss: 0.025329 | ΔLossTotal: -0.0213 | Iteration: 0031 | Time elapsed: 00h 00 m 04 s\n",
      "epoch  0  dataset  0  training Loss: 0.042598 | ΔLossTotal: -0.0040 | Iteration: 0032 | Time elapsed: 00h 00 m 04 s\n",
      "epoch  0  dataset  0  training Loss: 0.034184 | ΔLossTotal: -0.0125 | Iteration: 0033 | Time elapsed: 00h 00 m 04 s\n",
      "epoch  0  dataset  0  training Loss: 0.040416 | ΔLossTotal: -0.0062 | Iteration: 0034 | Time elapsed: 00h 00 m 04 s\n",
      "epoch  0  dataset  0  training Loss: 0.034305 | ΔLossTotal: -0.0123 | Iteration: 0035 | Time elapsed: 00h 00 m 05 s\n",
      "epoch  0  dataset  0  training Loss: 0.037048 | ΔLossTotal: -0.0096 | Iteration: 0036 | Time elapsed: 00h 00 m 05 s\n",
      "epoch  0  dataset  0  training Loss: 0.034918 | ΔLossTotal: -0.0117 | Iteration: 0037 | Time elapsed: 00h 00 m 05 s\n",
      "epoch  0  dataset  0  training Loss: 0.036370 | ΔLossTotal: -0.0103 | Iteration: 0038 | Time elapsed: 00h 00 m 05 s\n",
      "epoch  0  dataset  0  training Loss: 0.036407 | ΔLossTotal: -0.0102 | Iteration: 0039 | Time elapsed: 00h 00 m 05 s\n",
      "epoch  0  dataset  0  training Loss: 0.036808 | ΔLossTotal: -0.0098 | Iteration: 0040 | Time elapsed: 00h 00 m 05 s\n",
      "epoch  0  dataset  0  training Loss: 0.035985 | ΔLossTotal: -0.0107 | Iteration: 0041 | Time elapsed: 00h 00 m 05 s\n",
      "epoch  0  dataset  0  training Loss: 0.035334 | ΔLossTotal: -0.0113 | Iteration: 0042 | Time elapsed: 00h 00 m 05 s\n",
      "epoch  0  dataset  0  training Loss: 0.036210 | ΔLossTotal: -0.0104 | Iteration: 0043 | Time elapsed: 00h 00 m 05 s\n",
      "epoch  0  dataset  0  training Loss: 0.034025 | ΔLossTotal: -0.0126 | Iteration: 0044 | Time elapsed: 00h 00 m 06 s\n",
      "epoch  0  dataset  0  training Loss: 0.035518 | ΔLossTotal: -0.0111 | Iteration: 0045 | Time elapsed: 00h 00 m 06 s\n",
      "epoch  0  dataset  0  training Loss: 0.034181 | ΔLossTotal: -0.0125 | Iteration: 0046 | Time elapsed: 00h 00 m 06 s\n",
      "epoch  0  dataset  0  training Loss: 0.036046 | ΔLossTotal: -0.0106 | Iteration: 0047 | Time elapsed: 00h 00 m 06 s\n",
      "epoch  0  dataset  0  training Loss: 0.034258 | ΔLossTotal: -0.0124 | Iteration: 0048 | Time elapsed: 00h 00 m 06 s\n",
      "epoch  0  dataset  0  training Loss: 0.032635 | ΔLossTotal: -0.0140 | Iteration: 0049 | Time elapsed: 00h 00 m 06 s\n",
      "epoch  0  dataset  0  training Loss: 0.034268 | ΔLossTotal: -0.0124 | Iteration: 0050 | Time elapsed: 00h 00 m 06 s\n",
      "epoch  0  dataset  0  training Loss: 0.033724 | ΔLossTotal: -0.0129 | Iteration: 0051 | Time elapsed: 00h 00 m 06 s\n",
      "epoch  0  dataset  0  training Loss: 0.032896 | ΔLossTotal: -0.0138 | Iteration: 0052 | Time elapsed: 00h 00 m 07 s\n",
      "epoch  0  dataset  0  training Loss: 0.035200 | ΔLossTotal: -0.0114 | Iteration: 0053 | Time elapsed: 00h 00 m 07 s\n",
      "epoch  0  dataset  0  training Loss: 0.032491 | ΔLossTotal: -0.0142 | Iteration: 0054 | Time elapsed: 00h 00 m 07 s\n",
      "epoch  0  dataset  0  training Loss: 0.031373 | ΔLossTotal: -0.0153 | Iteration: 0055 | Time elapsed: 00h 00 m 07 s\n",
      "epoch  0  dataset  0  training Loss: 0.036685 | ΔLossTotal: -0.0100 | Iteration: 0056 | Time elapsed: 00h 00 m 07 s\n",
      "epoch  0  dataset  0  training Loss: 0.036052 | ΔLossTotal: -0.0106 | Iteration: 0057 | Time elapsed: 00h 00 m 07 s\n",
      "epoch  0  dataset  0  training Loss: 0.033613 | ΔLossTotal: -0.0130 | Iteration: 0058 | Time elapsed: 00h 00 m 07 s\n",
      "epoch  0  dataset  0  training Loss: 0.034732 | ΔLossTotal: -0.0119 | Iteration: 0059 | Time elapsed: 00h 00 m 07 s\n",
      "epoch  0  dataset  0  training Loss: 0.035853 | ΔLossTotal: -0.0108 | Iteration: 0060 | Time elapsed: 00h 00 m 08 s\n",
      "epoch  0  dataset  0  training Loss: 0.028809 | ΔLossTotal: -0.0178 | Iteration: 0061 | Time elapsed: 00h 00 m 08 s\n",
      "epoch  0  dataset  0  training Loss: 0.029029 | ΔLossTotal: -0.0176 | Iteration: 0062 | Time elapsed: 00h 00 m 08 s\n",
      "epoch  0  dataset  0  training Loss: 0.035927 | ΔLossTotal: -0.0107 | Iteration: 0063 | Time elapsed: 00h 00 m 08 s\n",
      "epoch  0  dataset  0  training Loss: 0.030683 | ΔLossTotal: -0.0160 | Iteration: 0064 | Time elapsed: 00h 00 m 08 s\n",
      "epoch  0  dataset  0  training Loss: 0.032270 | ΔLossTotal: -0.0144 | Iteration: 0065 | Time elapsed: 00h 00 m 08 s\n",
      "epoch  0  dataset  0  training Loss: 0.030672 | ΔLossTotal: -0.0160 | Iteration: 0066 | Time elapsed: 00h 00 m 08 s\n",
      "epoch  0  dataset  0  training Loss: 0.032361 | ΔLossTotal: -0.0143 | Iteration: 0067 | Time elapsed: 00h 00 m 08 s\n",
      "epoch  0  dataset  0  training Loss: 0.034772 | ΔLossTotal: -0.0119 | Iteration: 0068 | Time elapsed: 00h 00 m 09 s\n",
      "epoch  0  dataset  0  training Loss: 0.030541 | ΔLossTotal: -0.0161 | Iteration: 0069 | Time elapsed: 00h 00 m 09 s\n",
      "epoch  0  dataset  0  training Loss: 0.035451 | ΔLossTotal: -0.0112 | Iteration: 0070 | Time elapsed: 00h 00 m 09 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  dataset  0  training Loss: 0.032044 | ΔLossTotal: -0.0146 | Iteration: 0071 | Time elapsed: 00h 00 m 09 s\n",
      "epoch  0  dataset  0  training Loss: 0.031532 | ΔLossTotal: -0.0151 | Iteration: 0072 | Time elapsed: 00h 00 m 09 s\n",
      "epoch  0  dataset  0  training Loss: 0.033112 | ΔLossTotal: -0.0135 | Iteration: 0073 | Time elapsed: 00h 00 m 09 s\n",
      "epoch  0  dataset  0  training Loss: 0.031460 | ΔLossTotal: -0.0152 | Iteration: 0074 | Time elapsed: 00h 00 m 09 s\n",
      "epoch  0  dataset  0  training Loss: 0.031534 | ΔLossTotal: -0.0151 | Iteration: 0075 | Time elapsed: 00h 00 m 09 s\n",
      "epoch  0  dataset  0  training Loss: 0.029118 | ΔLossTotal: -0.0175 | Iteration: 0076 | Time elapsed: 00h 00 m 09 s\n",
      "epoch  0  dataset  0  training Loss: 0.032167 | ΔLossTotal: -0.0145 | Iteration: 0077 | Time elapsed: 00h 00 m 10 s\n",
      "epoch  0  dataset  0  training Loss: 0.030514 | ΔLossTotal: -0.0161 | Iteration: 0078 | Time elapsed: 00h 00 m 10 s\n",
      "epoch  0  dataset  0  training Loss: 0.032345 | ΔLossTotal: -0.0143 | Iteration: 0079 | Time elapsed: 00h 00 m 10 s\n",
      "epoch  0  dataset  0  training Loss: 0.032002 | ΔLossTotal: -0.0146 | Iteration: 0080 | Time elapsed: 00h 00 m 10 s\n",
      "epoch  0  dataset  0  training Loss: 0.031220 | ΔLossTotal: -0.0154 | Iteration: 0081 | Time elapsed: 00h 00 m 10 s\n",
      "epoch  0  dataset  0  training Loss: 0.032592 | ΔLossTotal: -0.0141 | Iteration: 0082 | Time elapsed: 00h 00 m 10 s\n",
      "epoch  0  dataset  0  training Loss: 0.030697 | ΔLossTotal: -0.0159 | Iteration: 0083 | Time elapsed: 00h 00 m 10 s\n",
      "epoch  0  dataset  0  training Loss: 0.028647 | ΔLossTotal: -0.0180 | Iteration: 0084 | Time elapsed: 00h 00 m 10 s\n",
      "epoch  0  dataset  0  training Loss: 0.030991 | ΔLossTotal: -0.0157 | Iteration: 0085 | Time elapsed: 00h 00 m 11 s\n",
      "epoch  0  dataset  0  training Loss: 0.026695 | ΔLossTotal: -0.0200 | Iteration: 0086 | Time elapsed: 00h 00 m 11 s\n",
      "epoch  0  dataset  0  training Loss: 0.025188 | ΔLossTotal: -0.0215 | Iteration: 0087 | Time elapsed: 00h 00 m 11 s\n",
      "epoch  0  dataset  0  training Loss: 0.032087 | ΔLossTotal: -0.0146 | Iteration: 0088 | Time elapsed: 00h 00 m 11 s\n",
      "epoch  0  dataset  0  training Loss: 0.033679 | ΔLossTotal: -0.0130 | Iteration: 0089 | Time elapsed: 00h 00 m 11 s\n",
      "epoch  0  dataset  0  training Loss: 0.028240 | ΔLossTotal: -0.0184 | Iteration: 0090 | Time elapsed: 00h 00 m 11 s\n",
      "epoch  0  dataset  0  training Loss: 0.032936 | ΔLossTotal: -0.0137 | Iteration: 0091 | Time elapsed: 00h 00 m 11 s\n",
      "epoch  0  dataset  0  training Loss: 0.031430 | ΔLossTotal: -0.0152 | Iteration: 0092 | Time elapsed: 00h 00 m 11 s\n",
      "epoch  0  dataset  0  training Loss: 0.030138 | ΔLossTotal: -0.0165 | Iteration: 0093 | Time elapsed: 00h 00 m 12 s\n",
      "epoch  0  dataset  0  training Loss: 0.030637 | ΔLossTotal: -0.0160 | Iteration: 0094 | Time elapsed: 00h 00 m 12 s\n",
      "epoch  0  dataset  0  training Loss: 0.026931 | ΔLossTotal: -0.0197 | Iteration: 0095 | Time elapsed: 00h 00 m 12 s\n",
      "epoch  0  dataset  0  training Loss: 0.032912 | ΔLossTotal: -0.0137 | Iteration: 0096 | Time elapsed: 00h 00 m 12 s\n",
      "epoch  0  dataset  0  training Loss: 0.028519 | ΔLossTotal: -0.0181 | Iteration: 0097 | Time elapsed: 00h 00 m 12 s\n",
      "epoch  0  dataset  0  training Loss: 0.025796 | ΔLossTotal: -0.0209 | Iteration: 0098 | Time elapsed: 00h 00 m 12 s\n",
      "epoch  0  dataset  0  training Loss: 0.028550 | ΔLossTotal: -0.0181 | Iteration: 0099 | Time elapsed: 00h 00 m 12 s\n",
      "epoch  0  dataset  0  training Loss: 0.026471 | ΔLossTotal: -0.0202 | Iteration: 0100 | Time elapsed: 00h 00 m 12 s\n",
      "epoch  0  dataset  0  training Loss: 0.036602 | ΔLossTotal: -0.0100 | Iteration: 0101 | Time elapsed: 00h 00 m 12 s\n",
      "epoch  0  dataset  0  training Loss: 0.029969 | ΔLossTotal: -0.0167 | Iteration: 0102 | Time elapsed: 00h 00 m 13 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6240\\952655365.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m#现在data和intdata都是tensor，分别存储smiles.txt进行ont-hot编码和序号编码后的结果，size如下输\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#Run on GPU if available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNo_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTDDTnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TDDTnetv4%d.pth\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6240\\1776149358.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(ep, No_data)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m#Intermediary saves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "epoch = 50\n",
    "for ep in range(epoch):\n",
    "    print(\"start the epoch %d training!\"%ep)\n",
    "    \n",
    "    for No_data in range(10):\n",
    "\n",
    "        print(\"prepare protein data\")\n",
    "         \n",
    "        protein_data = open('../data/train_protein_split%d.txt'%No_data, 'r').readlines()\n",
    "        protein_int_data = np.array([protein2int(i, protein_voc) for i in tqdm(protein_data)])\n",
    "    \n",
    "        input_data = np.load(\"../data/OneHot_input_Data%d.npz\"%No_data)\n",
    "        input_data = input_data[\"arr_0\"]\n",
    "        input_data_tensor = torch.from_numpy(input_data)\n",
    "        #.npz中保存的是smiles.txt对字符进行序号编码后的结果\n",
    "\n",
    "        output_data = np.load(\"../data/Int_output_Data%d.npz\"%No_data)\n",
    "        output_data = output_data[\"arr_0\"]\n",
    "        output_data_tensor = torch.from_numpy(output_data)\n",
    "\n",
    "        \n",
    "        dataset = TensorDataset(output_data_tensor, input_data_tensor)\n",
    "        train_loader = DataLoader(dataset = dataset, batch_size = 75, drop_last = True, shuffle = False)\n",
    "        #Load SMILES data as integer labels and as one-hot encoding\n",
    "        #将数据加载为整数标签和一个独热编码，ohesmiles.npz内保存的是smiles.txt经过one-hot后的结果\n",
    "        print(\"input_data size: \" + str(input_data_tensor.size()))             #data的三个维度，第0维表示字符，第1，2维是one-hot后的向量。\n",
    "        print(\"output_data size: \" + str(output_data_tensor.size()))  #intdata就是一个字符串，每一个字符代表一个smiles(的序号）。\n",
    "        #现在data和intdata都是tensor，分别存储smiles.txt进行ont-hot编码和序号编码后的结果，size如下输\n",
    "        #Run on GPU if available\n",
    "        train(ep,No_data)\n",
    "        \n",
    "    torch.save(TDDTnet.state_dict(), \"TDDTnetv4%d.pth\"%int(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(TDDTnet.state_dict(), \"TDDTnetv3F.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e817fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    " \n",
    "#     def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    " \n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         self.register_buffer('pe', pe)\n",
    " \n",
    "#     def forward(self, x):\n",
    "#         x = x + self.pe[:x.size(0), :]\n",
    "#         return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d84577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# protein_max_line = 0\n",
    "# try:\n",
    "#     file = open('../data/protein.txt', 'r')\n",
    "# except FileNotFoundError:\n",
    "#     print('Protein file is not found')\n",
    "# else:\n",
    "#     lines = file.readlines()\n",
    "#     for line in lines:\n",
    "#         if len(line)> protein_max_line:\n",
    "#             protein_max_line = len(line)\n",
    "\n",
    "# print(\"protein max line:\", protein_max_line)\n",
    "\n",
    "# protein_all = open('../data/protein.txt', \"r\").read()\n",
    "# protein_voc = list(set(protein_all))\n",
    "# protein_voc_num = len(protein_voc)\n",
    "\n",
    "# print(\"protein voc num:\", protein_voc_num)\n",
    "# print(\"protein voc\", protein_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0450dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = preprocessing.LabelEncoder()\n",
    "# protein_seq1 = \"PVMVLENIEPEIVYAGYDSSKPDTAENLLSTLNRLAGKQMIQVVKWAKVLPGFKNLPLEDQITLIQYSWMCLLSFALSWRSYKHTNSQFLYFAPDLVFNEEKMHQSAMYELCQGMHQISLQFVRLQLTFEEYTIMKVLLLLSTIPKDGLKSQAAFEEMRTNYIKELRKMVQRFYQLTKLLDSMHDLVSDLLEFCFYTFRESHALKVEFPAMLVEIISDQLPKVESGNAKPLYFH\"\n",
    "# protein_seq1_int = protein2int(protein_seq1, protein_voc)\n",
    "# print(protein_seq1_int)#\n",
    "\n",
    "# embedding = nn.Embedding(21, 32)\n",
    "# protein_seq1_array = np.asarray([eval(i) for i in protein_seq1_int])\n",
    "# print(\"array:\",protein_seq1_array)\n",
    "\n",
    "# protein_seq1_tensor = torch.from_numpy(protein_seq1_array).unsqueeze(0)\n",
    "# print(\"tensor size:\",protein_seq1_tensor.size())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
